nav:
  - Home: index.md
  - Why use LLM Compressor?: steps/why-llmcompressor.md
  - Compresssing your model, step-by-step:
    - Choosing your model: steps/choosing-model.md
    - Choosing the right compression scheme: steps/choosing-scheme.md
    - Choosing the right compression algorithm: steps/choosing-algo.md
    - Choosing a dataset: steps/choosing-dataset.md
    - Compressing your model: steps/compress.md
  - Deploying with vLLM: steps/deploy.md
  - Getting started:
    - getting-started/index.md
    - Installing LLM Compressor: getting-started/install.md
  - Key Models:
    - key-models/index.md
    - Llama 4:
      - key-models/llama4/index.md
      - FP8 Example: key-models/llama4/fp8-example.md
    - Qwen3:
      - key-models/qwen3/index.md
      - FP8 Example: key-models/qwen3/fp8-example.md
    - Kimi-K2:
      - key-models/kimi-k2/index.md
      - FP8 Example: key-models/kimi-k2/fp8-example.md
    - Mistral Large 3:
      - key-models/mistral-large-3/index.md
      - FP8 Example: key-models/mistral-large-3/fp8-example.md
  - Guides:
    - Compression Schemes: guides/compression_schemes.md
    - Saving a Model: guides/saving_a_model.md
    - Observers: guides/observers.md
    - Memory Requirements: guides/memory.md
    - Runtime Performance: guides/runtime.md
  - Examples:
    - examples/index.md
    - examples/*
  - Developer:
    - developer/index.md
    - developer/*
  - API Reference:
    - api/*
  - FAQ: 
    - faq/faq.md
